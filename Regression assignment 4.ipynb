{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56cca9-13e5-4539-835c-58cc4c66d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699b4b9-6d91-4040-8ebb-048ccc3971ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Lasso regression is a type of linear regression which adds a penalty which is equal to absoulute value \n",
    "    of magnitude of coefficients. This leads to coefficients being shrunk towards 0. The larger the coefficient\n",
    "    the more shrinking occurs.\n",
    "    Differences:\n",
    "        It can shrink some coefficients to directly zero, removing some feature aiding in feature selection.\n",
    "        Other regression models like ridge and linear do not remove variables but include all in the model.\n",
    "        Lasso regression leads to sparse models with fewer parameters which makes models more interpretable.\n",
    "        Lasso regression introduces bias into model but reduces the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a5924-c46d-4504-9fd7-af3702720ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff479b-17bc-47bf-9879-4fc7e96355ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. The main advantage of Lasso regression is that it performs automatic feature selection and outputs a \n",
    "    sparse model. \n",
    "    It shrinks coefficients of less important features to exactly zero, removing those features all together.\n",
    "    This performs like feature selection process.\n",
    "    The output is sparse model with relatively less features and more interpretable.\n",
    "    Lasso regularisation is computationally efficient and scalable to very high dimensional datasets with vast\n",
    "    number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c167b8-d26d-403d-a65c-da0d7d09c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472b1c0-8d00-4c2f-ad68-47dc30028ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. The coefficients shrunken to 0 indicate that the feature was removed by Lasso regularisation.\n",
    "    Larger non zero coefficients indicate features that are more important in predicting the target variable. \n",
    "    The sign of non zero coefficient shows the direction of relation of that feature with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc5168-b7e4-42ea-a9a8-f5681cec4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0ec94-848e-4f77-a545-f84d7e3c17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. The tuning parameters which can be adjusted in Lasso regression are alpha and normalization parameter.\n",
    "    Alpha - This is the regularization strength parameter. A higher alpha leads to stronger regularization \n",
    "    which means more coefficients are shrunk to zero. This reduces model's complexity but increases bias. \n",
    "    \n",
    "    Normalization parameter - Lasso regularization includes an option to normalize predictor variables. The \n",
    "    normalization approach used affects the resulting coefficients.\n",
    "    \n",
    "    Adjusting the parameters:\n",
    "        Alpha - Higher alpha increases bias but reduces variance and complexity. Lower alpha reduces bias but \n",
    "        may overfit if too low. \n",
    "        Normalization - Appropriate normalization can improve conditioning of the problem and coefficient \n",
    "        stability. Lack of normalization can vary coefficients based on scaling of raw predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67d96d-800c-4d3b-8fac-1d052cc4c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f4f1b-65ed-4357-93f4-1996d21b2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Yes, Lasso regression can be used for non linear regression problems. \n",
    "    The linear lasso model can be applied to non linear functions of predictors known as basis expansions. Like\n",
    "    adding polynomial terms, interactions, splines etc expands the feature space to capture non - linear relations.\n",
    "    Additive models estimates a lasso model seperately on non linear smoothed functions of each predictor and \n",
    "    additive combination of these non linear functions provide a non linear model.\n",
    "    The lasso penalty can be added to the loss function of neural network model. This helps perform variable\n",
    "    selection and regularization for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725a91a-4a6c-4c8a-b5cd-d13aba881201",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19c3cb-14d3-4986-82d5-a18177407b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Ridge uses L2 penalty while Lasso uses L1 penalty.\n",
    "    The L2 penalty in Ridge shrinks few coefficients near to zero but not zero while L1 penalty in Lasso \n",
    "    shrinks few coefficients exactly to zero.\n",
    "    Ridge has higher bias but lower variance compared to Lasso. Lasso exhibits more bias but can have less\n",
    "    variance due to feature selection.\n",
    "    The Lasso model is more interpretable as it performs feature selection and removes few features while\n",
    "    Ridge keeps all features.\n",
    "    There is no superiority among both. Ridge may outperform Lasso in some contexts like highly correlated \n",
    "    variable but Lasso is preferable in feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78996ef6-7e33-4bcf-9f3b-e7671a77fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b2d9e-50d8-46a2-8f6b-22b4860cb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Yes Lasso can handle multicollinearity. \n",
    "    The L1 penalty shrinks few coefficients towards zero, effectively reducing model reliance on any single \n",
    "    correlated predictor. \n",
    "    In presence of correlated predictors lasso tends to pick one variable from a correlated group and drive \n",
    "    the coefficients of the other to zero which removes the redundant features.\n",
    "    Correlated coefficients are unstable in linear regression, the shrinkage and variable selection of lasso\n",
    "    results in more stable coefficient estimates.\n",
    "    The selected variables have more stable interpretations about relationships to the target compared to \n",
    "    unstable correlated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e50cf-4e8a-4acd-8edf-0d03ef384aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b914f-2fad-4f9e-bdb3-40128bf3bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. We can choose the value of lambda by below techniques:\n",
    "    Cross Validation - The lasso model is trained using different lambda values and the one with lowest \n",
    "    cross validation error is chosen.\n",
    "    Information criteria - Criteria like AIC, BIC that balance model fit and complexity can be used. The lambda\n",
    "    value that optimizes the criteria is chosen.\n",
    "    Grid search - An exhaustive grid search trying different lambda values in a loop and evaluating the model \n",
    "    performance can be done to choose optimal lambda. \n",
    "    Stability selection - Lasso models are trained on subsamples for different lambda. The lambda value with most\n",
    "    stable variable selection is optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
